# LLMの概要と仕組み

## 大規模言語モデル（LLM）とは

- 大量のテキストデータから学習した言語モデル
- 自然言語を理解し、生成する能力を持つ
- 数十億から数千億のパラメータを持つ大規模なニューラルネットワーク
- 様々なタスクに対応可能な汎用的な言語理解・生成能力

## LLMの歴史的発展

- 2018年：BERT（Bidirectional Encoder Representations from Transformers）
- 2019年：GPT-2（Generative Pre-trained Transformer 2）
- 2020年：GPT-3の登場
- 2022年：ChatGPTの公開
- 2023年：GPT-4、Claude、Geminiなどの登場

## LLMの基本的な仕組み

### トークン化（Tokenization）
- テキストを小さな単位（トークン）に分割
- 単語、部分単語、文字などの単位
- 語彙サイズは数万から数十万トークン

### 埋め込み（Embedding）
- トークンを数値ベクトルに変換
- 意味的に近い言葉は近いベクトル表現に
- 言語の意味的関係を数学的に表現

### Transformerアーキテクチャ
- 自己注意機構（Self-Attention）が核となる技術
- 文脈を考慮した並列処理が可能
- エンコーダー・デコーダー構造

### 事前学習と微調整
- 事前学習（Pre-training）：大量のテキストデータで学習
- 微調整（Fine-tuning）：特定のタスク向けに調整
- インストラクション微調整（Instruction Fine-tuning）

## 主要なLLMの種類

### GPTシリーズ（OpenAI）
- GPT-3.5：ChatGPTの基盤モデル
- GPT-4：マルチモーダル能力を持つ高性能モデル

### Claude（Anthropic）
- 安全性と有用性を重視したモデル
- 長いコンテキスト処理が得意

### Gemini（Google）
- マルチモーダル能力を持つモデル
- 複雑な推論能力に優れる

### Llama（Meta）
- オープンソースモデル
- 研究・商用利用が可能

## LLMの能力と限界

### 得意なこと
- 自然な文章生成
- 質問応答
- 要約・翻訳
- コード生成

### 限界
- 最新情報の欠如（学習データの制限）
- 事実の「幻覚」（hallucination）
- 複雑な推論の不確実性
- バイアスの存在

## 次回予告：プロンプトエンジニアリング基礎

- プロンプトとは何か
- 効果的なプロンプト設計の原則
- プロンプトパターンとテクニック
- プロンプト最適化の方法 