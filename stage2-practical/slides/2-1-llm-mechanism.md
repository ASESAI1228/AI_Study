# LLMの仕組みと特性

## LLMの基本原理

- 大規模言語モデル（Large Language Model）の略
- 膨大なテキストデータから言語パターンを学習
- 次の単語（トークン）を予測する能力を基盤とする
- 数十億〜数千億のパラメータを持つ大規模ニューラルネットワーク

## Transformerアーキテクチャ

- 2017年に発表された「Attention is All You Need」論文で提案
- 自己注意機構（Self-Attention）が核となる技術
- 並列処理が可能で学習効率が高い
- エンコーダー・デコーダー構造

### 自己注意機構の仕組み
- 文脈内の単語間の関連性を計算
- 各単語が他のすべての単語とどれだけ関連しているかを数値化
- 長距離依存関係を効率的に捉えられる

## LLMの学習プロセス

### 事前学習（Pre-training）
- インターネット上の膨大なテキストデータを使用
- 教師なし学習で言語の構造とパターンを学習
- マスク言語モデリング（BERT）や自己回帰モデリング（GPT）

### 微調整（Fine-tuning）
- 特定のタスクや領域向けに調整
- 少量の教師あり学習データを使用
- 基本的な言語理解能力を保持しながら特定タスクに最適化

### 強化学習（RLHF）
- 人間のフィードバックによる強化学習
- 有用で安全な回答を生成するよう調整
- 人間の価値観や倫理観を反映

## トークン化と埋め込み

### トークン化
- テキストを小さな単位（トークン）に分割
- 単語、部分単語、文字などの単位
- 言語によって分割方法が異なる
- 例：「契約書」→「契約」+「書」または「契約書」

### 埋め込み（Embedding）
- トークンを高次元ベクトル空間に変換
- 意味的に近い単語は近い位置に配置される
- 単語間の関係性を数学的に表現
- 例：「弁護士」と「法律」は近い位置に、「弁護士」と「料理」は遠い位置に

## コンテキストウィンドウ

- LLMが一度に処理できるトークン数の上限
- GPT-3.5：約4,000トークン
- GPT-4：約8,000〜32,000トークン
- Claude：約100,000トークン
- 長い文書の処理には分割や要約が必要

## 法務分野におけるLLMの特性

- 膨大な法的テキストから学習している
- 法律用語や法的概念の基本的理解がある
- 文脈を考慮した法的文書の生成・分析が可能
- ただし、最新の法改正や特定の判例の詳細は学習していない場合がある

## 次回予告：LLMの強みと弱み

- LLMが得意とするタスク
- LLMの限界と注意点
- 法務業務における適切な活用場面 